---
title: 'The game of MasterMind in 2020: an overview of literature'
author: "JJ Merelo"
date: "4 de marzo de 2020"
output: pdf_document

bibliography: mm.bib
---

Mastermind [@wiki_mm] is a deduction game where one user has to find out a length-4 or length-6 combinations of colored pegs by adventuring possible combinations and getting answers that tell the user how many pegs, not which ones, are in the right position, and how many got the color right but missed the position.

Google Scholar returns, in all, 57 articles that mention the words Mastermind and game in their title. That meager number does not really reflect the interest from all kind of fields this game has arisen. Since 2016, 8 papers have used the words in the title, every one of them with a different approach to solving the game or using it for other purposes, from teaching [@perucca2018tactile] to psychological evaluation [@hornik2017solution].

In general, there are four different axes the investigation related to Mastermind has gone. The first one is trying to solve the game itself. The game, or versions of it, have been generalized to what are called black box optimization problems. The third axe is related to using implementations of the game, or parts of the game, with pedagogic purposes; finally, a fourth axis is related to using the game to approach the way mental models are created. Our focus is on the first, so we will make a small overview of the rest first, to proceed finally to current solutions to the Mastermind puzzle.

This short overview does not pretend to be an exhaustive guide to the algorithmic world of mastermind; probably the best place to start there is our paper published in 2012 [ @MereloArxiv]. Our intention is rather to show current directions in the algorithmic world of mastermind, and what are the main lines of research in the last 5-7 years and where they originate.

## Black box optimization problems and their relation with Mastermind

In Mastermind, there's a "black box" (the codebreaker) that tells you the *fitness* of your submitted string by answering with black and white pegs; if we just look at the function itself that returns the answer, our objective will be to get that function to its optimum, that will tell us that we found out the whole combination; however, we don't really know the shape of that function, In the case of mastermind it will be the string difference between the code and our string; but we can generalize that problem to talk about black box optimization when all we know about the function is the result of certain queries. By putting the emphasis on the queries themselves, and how they can be generated once response to those queries is set in, the simple genetic algorithm can be analyzed theoretically (as done initially by Oliveto et al. in [@oliveto2012analysis]) so that lower bounds to the number of queries (evaluations) can be computed; if you consider a variation of mastermind where there are only two colors (red and blue), the answer tells you how many blue pegs are in the right position, and the string to find consists of only blue pegs, there's the relationship between Mastermind and evolutionary algorithms at large. This relationship has been studied further to this day, with an excellent proposition and review published just recently by Carola Doerr [@doerr2020complexity].

However, eliminating white pegs in the answer makes the problem much more difficult; at the same time, using shorter alphabets make the problem easier, since the search space is smaller. But at the end of the day, the essential problem of trying to find an optimal sequence of queries that is able to minimize the function is still the same, and it boils down to using queries that are able to extract maximal information from the black box. 

## Learning and other mental models

At the end of the day, Mastermind is a game; and as in any game, players have to elaborate an strategy to find solutions to the game. Using teaching tools like the ones proposed by Fiore et al. [@fiore2018tactile] will help users create such strategy, at the same time they can understand how to follow an algorithm, in this case Knuth's original algorithm, which uses scoring strategies on the set of feasible combinations to decide which one should be played [@Knuth]. A similar solution is offered by [@hornik2017solution], which implements Mastermind in the well known framework Scratch, widely used for teaching programming to kids. Even if it's a relatively complicated problem, implementing it helps students think about how to divide a problem in different parts, and even how to show a nice visual representation of the result, as well as a more analytical view of what's going on under the hood.

How people solve the game can be used to study cognitive models: [@zhao2018predicting] uses a game similar to MasterMind, called Deductive Mastermind and their results by a group of students, to find out the difficulty of the game; a technique called Dynamic Epistemic Logic (DEL), which is based on the elimination of states by updating a model, is used to analyze that; the same game is analyzed further in [@trutescu_raijmakers_2019]. This DEL is similar to an exhaustive search algorithm, which checks out combinations and is, in fact, an interesting strategy as long as you can hold the whole set of strings in memory; these approaches to Mastermind were researched in [@MereloArxiv]. The fact that, as proved by the paper, this seems the model that's actually used by people (as opposed to another model, called TABL, which reasons based on specific cases), indicates that there would be a limit to the size of Mastermind that could be played successfully by a human; but also that humans follow strategies that can be similar to algorithms, at least for some of them.

Another variant of Mastermind, called Entropy Mastermind, has been used by Schulz et al. [@schulz2019exploring] to find out the strategies people use to explore spaces by formulating queries. Curiously enough, the conclusion that humans work better when entropy in the game environment is low contrasts with current strategies for solving Mastermind, which actually use entropy to score solutions, playing those that have optimal entropy. In general, low entropy in a query will mean that the information extracted from the oracle that watches over the secret code is going to be low also.

## Solving Mastermind

For me, the most interesting axis of investigation is trying to find the solution by playing the game. An algorithm that tries to play in the same way we would will try to use hints to generate new combinations so that eventually the solution is found.

It seems reasonable to always use hints to play the next combination. A combination that meets all hints made by the codemaker is *plausible*, and possibly the hidden code. The initial strategy proposed by [@Knuth] in the seventies did precisely that.

There are, however, two problems with that strategy. The first one is that it converts one problem into another: finding plausible solutions. The second is that there's no principled way to decide which one of these solutions is the best, that is, is able to obtain maximum information from the oracle-codemaker. This second problem includes also the "first combination" problem: which combination needs to be played first, in order to find the solution fast.

As a matter of fact, both problems are related. Sometimes you need to relax the constraint to play plausible solutions in order to find a good solution. For instance, [@hornik2017solution] played following some heuristics; also, in [@jj-ppsn96], playing unplausible solutions helped generate new constraints that allowed to find the solution faster.

Most algorithms that try to play optimally, however, do play plausible solutions, some authors simply play consistent solutions as soon as they find them [@partynski2014cluster,@genmm99]; however, most authors focus in finding a good scoring strategy that establishes a ranking among existing plausible solutions to play just the best (given restrictions, of course). This scoring relies on using every solution in the plausible set (or a sample of it) as the code, and finding how it scores against the rest of the solutions. That generates a partition on the set of solutions, grouping all those that have the same response. In function of that partition, we will assign the score following different strategies [@MereloArxiv], based on the number of partitions, the size of the bigger partition, or even the entropy of the partition or number of partitions. This strategy, called Most Parts, was the one chosen by Knuth initially; however, different authors have used different strategies. In [@mm14], genetic programming was used to generate different scoring functions, achieving some success; however, in [@MereloArxiv] exhaustive search algorithms were examined together with these strategies, concluding that no one strategy was going to be able to beat the others: combinations of them to score combinations worked better; however, which combination or single strategy was the best depended on the size of the problem; size of the problem that has been lately bound by Doerr et al. paper [@Doerr_Acm_16], which found that the codebreaker should be able to find the solution in O(n log log n); Peczarski [@peczarski2018algorithm] do try to find out a similar bound, although limited to what is called an AB game, equivalent to Mastermind with two pegs (A and B). They mention that their proof could be extended to MasterMind proper, but apparently that has not happened so far. Independently, El Ouali et al. published another paper [@g9010002] that focuses on the same problem, finding bounds for several specific cases.

Without a principled approach to find that strategy, latest papers focus more on speed and performance than on proposing new formulas to find the solution, some strategies that are tagged *new*, as the one proposed by Rhodes in [@rhodes2019search] and called MERC, are but a variant of the basic set of consistent-set reduction strategies outlined above.  Oijen's BSc Thesis [@oijen2018genetic] tests different algorithms following on Berghmann's footsteps [@Berghman20091880] trying to find an algorithm that scales better, and that eventually uses genetic local search to find optimal solutions.

A paper by Lu et al. [@lu2017playing] uses reinforcement learning to try and learn the best scoring strategy to select the solution that is going to be played using something called Sarsa(Lambda) to look for strategies that try to find that code. They affirm that this strategy is able to lower the average number of guesses needed to 4.294. This is lower than the previous optimum published in [@MereloArxiv], which was 4.404, proving that adaptive scoring strategies might be better than single scoring strategies based on partition. They are not exactly comparable, however, since apparently the codes were selected in a certain way, instead of being an average across all combinations used as a secret code.

Among these approaches, we have to mention some theoretical attacks to the game that use either variants of the game or variants of the problem to find solution. One such variant of the problem is the so called Mastermind Satisfiability Problem: given a set of combinations and their responses, is there a secret code that satisfies all of them? Collina in [@collina2018fastermind] proposes to map this problem to the SAT problem, and solving it using SAT solvers. It happens to obtain very good results for high dimensions, not so good for the smaller-size problems. Bonnet and Viennot [@bonnet2016nash] try to find good strategies for the codemaker; while most solving methods try to get a good average across all possible random solutions, if the codemaker follows an optimal strategy too these methods might fall in the worst case and be defeated. Another variant uses a different distance between the played and secret code [@fern2019query], which besides can be played nosily, with distances returned having a certain amount of randomness. These papers contribute mainly to more serious applications of Mastermind, such as differential cryptanalisis, but it's difficult to apply their conclusions to actually solving the game.



## References